{"cells": [{"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["# Classification Review"]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": "Placeholder"}, "outputs": [], "source": ["import pandas as pd\n", "df = pd.read_csv('data/aug_train.csv')\n", "df.head()"]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": 2}, "outputs": [], "source": ["import pandas as pd\n", "df = pd.read_csv('data/aug_train.csv')\n", "df.head()"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["## 1. Seperate the features from the target.\n", "* Assign the features to `X`.\n", "* Assign the target to `y`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": "Placeholder"}, "outputs": [], "source": ["# Your code here"]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": 5}, "outputs": [], "source": ["X = df.drop('Response', axis = 1)\n", "y = df.Response"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["## 2. Drop the `id` column from `X`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": "Placeholder"}, "outputs": [], "source": ["# Your code here"]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": 8}, "outputs": [], "source": ["X = X.drop('id', axis = 1)"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["## 3. Create a train test split\n", "> Set the random state to `2021`."]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": "Placeholder"}, "outputs": [], "source": ["# Your code here"]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": 11}, "outputs": [], "source": ["from sklearn.model_selection import train_test_split\n", "\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2021)"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["## 4. Select the numeric features"]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": "Placeholder"}, "outputs": [], "source": ["# Your code here"]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": 14}, "outputs": [], "source": ["X_train_numeric = X_train.select_dtypes('number')\n", "X_test_numeric = X_test[X_train_numeric.columns]"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["## 5. Import a scaler and scale the data"]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": "Placeholder"}, "outputs": [], "source": ["# Your code here"]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": 17}, "outputs": [], "source": ["from sklearn.preprocessing import StandardScaler\n", "\n", "scaler = StandardScaler()\n", "scaler.fit(X_train_numeric)\n", "X_train_scaled = scaler.transform(X_train_numeric)\n", "X_test_scaled = scaler.transform(X_test_numeric)"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["## 6. Initialize a logistic regression model\n", "* Set the random state to `2021`"]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": "Placeholder"}, "outputs": [], "source": ["# Your code here"]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": 20}, "outputs": [], "source": ["from sklearn.linear_model import LogisticRegression\n", "\n", "model = LogisticRegression(random_state=2021)"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["## 7. Fit the model to the scaled data"]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": "Placeholder"}, "outputs": [], "source": ["# Your code here"]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": 23}, "outputs": [], "source": ["model.fit(X_train_scaled, y_train)"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["## 8. Plot a confusion matrix"]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": "Placeholder"}, "outputs": [], "source": ["# Your code here"]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": 26}, "outputs": [], "source": ["from sklearn.metrics import plot_confusion_matrix\n", "\n", "plot_confusion_matrix(model, X_train_scaled, y_train);"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["## 9. Please calculate the accuracy score."]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": 28}, "outputs": [], "source": ["from sklearn.metrics import accuracy_score\n", "score = accuracy_score(y_train, model.predict(X_train_scaled))\n", "print('{:.2}'.format(score))"]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": "Placeholder"}, "outputs": [], "source": ["# Run this cell unchanged\n", "from src.questions import question_9\n", "question_9.display()"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["## 10. Please calculate the precision score."]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": 31}, "outputs": [], "source": ["from sklearn.metrics import precision_score\n", "score = precision_score(y_train, model.predict(X_train_scaled))\n", "print('{:.2}'.format(score))"]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": "Placeholder"}, "outputs": [], "source": ["# Run this cell unchanged\n", "from src.questions import question_10\n", "question_10.display()"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["## 11. Please calculate the recall score."]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": 34}, "outputs": [], "source": ["from sklearn.metrics import recall_score\n", "score = recall_score(y_train, model.predict(X_train_scaled))\n", "print('{:.2}'.format(score))"]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": "Placeholder"}, "outputs": [], "source": ["# Run this cell unchanged\n", "from src.questions import question_11\n", "question_11.display()"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["## 12. Please calculate the f1 score."]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": 37}, "outputs": [], "source": ["from sklearn.metrics import f1_score\n", "score = f1_score(y_train, model.predict(X_train_scaled))\n", "print('{:.2}'.format(score))"]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": "Placeholder"}, "outputs": [], "source": ["# Run this cell unchanged\n", "from src.questions import question_12\n", "question_12.display()"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["## 13. Multiple Choice\n", "\n", "We are working on a modeling project, and have determined that false positives are the most costly outcome. An ideal metric for this project is:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": "Placeholder"}, "outputs": [], "source": ["# Run this cell unchanged\n", "from src.questions import question_13\n", "question_13.display()"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["## 14. Multiple Choice\n", "\n", "We are working on a modeling project, and have determined that false negatives are the most costly outcome. An ideal metric for this project is:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": "Placeholder"}, "outputs": [], "source": ["# Run this cell unchanged\n", "from src.questions import question_14\n", "question_14.display()"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["## 15. Multiple Choice\n", "We are working on a modeling project with imbalanced data, and do not have a strong preference between false positives and false negatives. An ideal metric for this project is:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": "Placeholder"}, "outputs": [], "source": ["# Run this cell unchanged\n", "from src.questions import question_15\n", "question_15.display()"]}, {"cell_type": "markdown", "metadata": {"index": "Placeholder"}, "source": ["## 16. Multiple Choice\n", "\n", "We are working on a modeling project with balanced data, and do not have a strong preference between false positives and false negatives. An ideal metric for this project is:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"index": "Placeholder"}, "outputs": [], "source": ["# Run this cell unchanged\n", "from src.questions import question_16\n", "question_16.display()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.4"}}, "nbformat": 4, "nbformat_minor": 4}